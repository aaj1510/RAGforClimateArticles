{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3028396",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "\n",
    "#should be 3.9 and above to run project successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "print(PIL.__version__)\n",
    "\n",
    "#if your PIL is < 9.1.0 version, need to upgrade it.\n",
    "#pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9c205",
   "metadata": {},
   "source": [
    "Install dependencies above if you haven't done so (found in requirements.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ff05f",
   "metadata": {},
   "source": [
    "#    Import Langchain libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47c80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores import Chroma - discontinued\n",
    "#from langchain_community.chat_models import ChatOllama - discontinued\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a2fea5-71f1-48b5-8cb6-fa5ebef430c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf521e",
   "metadata": {},
   "source": [
    "#    Split PDF into bitesized chunks, so AI can ingest easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034cb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    loader = PyPDFLoader(\"CNET0_sg.pdf\")\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1024,\n",
    "        chunk_overlap = 100, \n",
    "        length_function = len,\n",
    "        add_start_index = True, \n",
    "    )\n",
    "    #split_documents is a langchain method. Alternatively, can also use create_documents  \n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "    print(f\"Split {len(pages)} documents into {len(chunks)} chunks.\")\n",
    "    \n",
    "    #Generate vector embedding for each chunk\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    #Create vector store - aka create chroma db from the pdf doc.\n",
    "    #Create db on the disk to make it modular for chatbots/cloud if uw\n",
    "    Chroma.from_documents(documents=chunks,  embedding=embedding, persist_directory=\"./sql_chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7508877",
   "metadata": {},
   "source": [
    "More about langchain's text splitting: \n",
    "[1](https://www.reddit.com/r/LangChain/comments/170mfkc/recursivecharactertextsplitter_create_documents/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9686e480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5 documents into 12 chunks.\n"
     ]
    }
   ],
   "source": [
    "ingest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1acd0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#Open API key file in read mode, and read\n",
    "file = open(\"hugging_face_API.txt\", \"r\") #replace with your API key from huggingface\n",
    "content = file.read()\n",
    "\n",
    "access_token_read = content\n",
    "access_token_write = content\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70912e7",
   "metadata": {},
   "source": [
    "# Creating a RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034c4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain():\n",
    "    model = ChatOllama(model=\"llama3\")\n",
    "    #\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        <s> [Instructions] You are a friendly assistant. Answer the question based only on the following context. \n",
    "        If you don't know the answer, then reply, No Context available for this question {input}. [/Instructions] </s> \n",
    "        [Instructions] Question: {input} \n",
    "        Context: {context} \n",
    "        Answer: [/Instructions]\n",
    "        \"\"\"\n",
    "    )\n",
    "    #Load vector store\n",
    "    embedding = FastEmbedEmbeddings()\n",
    "    vector_store = Chroma(persist_directory=\"./sql_chroma_db\", embedding_function=embedding)\n",
    "\n",
    "    #Create chain by using the vector store as a retriever obj\n",
    "    #Retriever will search for docs based on similarity score >=0.5, up to 3 docs\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 3,\n",
    "            \"score_threshold\": 0.5,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    #create document change using llama3 and prompt tempalte\n",
    "    document_chain = create_stuff_documents_chain(model, prompt)\n",
    "    \n",
    "    #create chain using document, and query\n",
    "    chain = create_retrieval_chain(retriever, document_chain)\n",
    "    \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b77bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query: str):\n",
    "    chain = rag_chain()\n",
    "    # invoke chain\n",
    "    result = chain.invoke({\"input\": query})\n",
    "    # print results\n",
    "    print(result[\"answer\"])\n",
    "    for doc in result[\"context\"]:\n",
    "        print(\"Source: \", doc.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6d9129-8d59-4026-afc0-9fd66307a7f5",
   "metadata": {},
   "source": [
    "# Ask a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c262e36b-497c-48da-8f34-8852fd50f7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, CNET50 makes soil testing more accessible for community gardeners and low-income households by bringing real-time, accessible soil insights to elderly residents, low-income households, and community growers through collaborations with groups like Edible Garden City, NParks Allotment Gardens, and social farming collectives. This allows community gardeners to make better decisions on fertilization and crop choices, leading to improved yields, lower costs, potential revenue streams, and more sustainable practices.\n",
      "Source:  CNET0_sg.pdf\n",
      "Source:  CNET0_sg.pdf\n",
      "Source:  CNET0_sg.pdf\n"
     ]
    }
   ],
   "source": [
    "ask(\"How does CNET50 make soil testing more accessible for community gardeners and low-income households?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f21424a-1a71-421a-857d-01de5299e3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Context available for this question. Since the provided context only talks about CNET50, a portable soil health and carbon measurement device, its relationship with UNSDG 10 (Reduced Inequalities) is not explicitly mentioned. Therefore, I cannot provide an answer based on the given information.\n",
      "Source:  CNET0_sg.pdf\n",
      "Source:  CNET0_sg.pdf\n",
      "Source:  CNET0_sg.pdf\n"
     ]
    }
   ],
   "source": [
    "ask(\"In what ways does the CNET50 initiative support the UNSDG 10 (Reduced Inequalities)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c954b-3641-404a-9070-6a5c86214df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging tips: \n",
    "\n",
    "1) Ollama server should be running when before ask query\n",
    "2) Ensure document is in text pdf, not images. pypdf reads text based pdfs\n",
    "3) Check dependencies if installed to environment "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
