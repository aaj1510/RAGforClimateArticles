üîç ClimateRAG: A RAG Pipeline for Climate Article Analysis
ClimateRAG is a Retrieval-Augmented Generation (RAG) pipeline designed to assist in querying and understanding climate-related articles. This is also specific to my use case, as i tend to do much research on climate-tech. Of course, any document can be fed to achieve similar results. 

üöß Note: This project is currently in development towards improving it's features.

üß† Project Purpose
Climate literature is vast and growing. This RAG implementation would help distill knowledge from content fed, by combining retrieval-based search with generation using Llama3. This way, instead of relying solely on static search or generic AI models, answers are generated from the pdf content. 

üèóÔ∏è Current Tech Stack
LangChain ‚Äî Framework for chaining together LLM workflows

Ollama ‚Äî Lightweight LLM runner for local model inference

HuggingFace Transformers ‚Äî For model loading and tokenisation

ChromaDB ‚Äî Open-source vector database used to store and retrieve document embeddings, easy to use and setup

Python ‚Äî Core implementation language

LLaMA 3 (via Ollama) ‚Äî Used for generating grounded answers from retrieved content, ideal for this use-case.

üß©Steps:

 Load and preprocess pdf/text file via langchain's splitting documents method

 Embed documents using HuggingFace embeddings, tokenisation and verification of llama3 model

 Store and query documents via ChromaDB (vector database)

 Retrieve relevant chunks with similarity search of >=0.5, no more than k = 3 docs

 Use a prompt template for standardized question answering

 llama3 generation strictly grounded in retrieved content

 Ensure ollama is running, and then pull the desired model from ollama (llama3). More instructions in notebook

 üß†**Improvements I'm working on**:
 Refine prompt templates for accuracy and consistency

 Build a front-end or API for user interaction, able to upload multiple docs at a time

 Fine-tuning implementation 

üêõ Some things i learnt and experienced:
llama3 model could not answer some questions I had, though it performed relatively well considering it's size. This is a wip. 
llama3 runs slowly, my device's RAM is limited in that sense. On a 16GB RAM minimally, it would improve performance significantly. 
Prompts need to be better crafted, so that llama can be more accurate
